---
title: "sommelieR"
author: "Seoyoon Cho, Paloma Hauser, Taylor Lagler, Mike Nodzenski, Bryce Rowland"
date: "4/24/2019"
output: 
  beamer_presentation:
    color: "seahorse"
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
---

```{r libraries, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# create the datasets for training and testing for the group project
set.seed(13)

library(caret)
library(readr)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(Hmisc)
library(corrplot)
library(tidyverse)
library(kableExtra)
library(knitr)
library(devtools) 
library(psych)

#install sommelieR
install_github("group-wine/sommelieR")
library(sommelieR)
```

```{r useful_functions, include = F}
pred_summary_stats <- function(preds, true_values, model){
  confusion_matrix_obj <- confusionMatrix(as.factor(preds),
                                      as.factor(true_values))
  accuracy <- confusion_matrix_obj$overall[1]
  kappa <- confusion_matrix_obj$overall[2]
  wt.kappa <- cohen.kappa(confusion_matrix_obj$table)$weighted.kappa
  
  
  pct_correct_by_cat <- (diag(confusion_matrix_obj$table)/table(true_values))
  pct_correct_df <- as.data.frame(pct_correct_by_cat) %>% 
    spread(true_values, Freq)
  pred_sum_stats <- bind_cols("Model" = model,"Prediction Accuracy" = accuracy,Kappa = kappa, `Weighted Kappa` = wt.kappa, pct_correct_df) %>% 
    mutate_at(.vars = vars(-c("Model", "Kappa", "Weighted Kappa")),
              .funs = function(x){x*100})
  return(pred_sum_stats)
}

```


```{r setup, include = F}
#load the datasets 
data("red_test")
data("red_train")
data("white_test")
data("white_train")

white.obs <- white_test$quality
red.obs <- red_test$quality

#center and scale the predictors 
red_test[ , colnames(red_test) != "quality"] <- apply(red_test[ , colnames(red_test) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
red_train[ , colnames(red_train) != "quality"] <- apply(red_train[ , colnames(red_train) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
white_test[ , colnames(white_test) != "quality"] <- apply(white_test[ , colnames(white_test) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
white_train[ , colnames(white_train) != "quality"] <- apply(white_train[ , colnames(white_train) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))

#--------------------------------------------------------------------#
# Data set up for boxplot code:
red_all = rbind(red_test, red_train)
red_all$quality <- as.factor(red_all$quality)
names(red_all) = gsub("\\.", "_", names(red_all))

white_all = rbind(white_test, white_train)
white_all$quality <- as.factor(white_all$quality)
names(white_all) = gsub("\\.", "_", names(white_all))

#--------------------------------------------------------------------#
# Data set up for RF classification by type code:

# merge data sets, add type indicator
type <- c(rep("red", dim(red_train)[1]),rep("white", dim(white_train)[1]))
all_train_a <- rbind(red_train, white_train)
all_train <- cbind(all_train_a, type)

type2 <- c(rep("red", dim(red_test)[1]),rep("white", dim(white_test)[1]))
all_test_a <- rbind(red_test, white_test)
all_test <- cbind(all_test_a, type2)

# convert type and quality to factor
all_train$type <- as.factor(all_train$type)
all_train$quality <- as.factor(all_train$quality)

all_test$type <- as.factor(all_test$type)
all_test$quality <- as.factor(all_test$quality)


x.train = as.data.frame(all_train[,1:11], ncol=11)
y.train = factor(all_train$type)

x.test = as.data.frame(all_test[,1:11], ncol=11)
y.test = factor(all_test$type)

#--------------------------------------------------------------------#
# Data set up for RF classification by quality code:
#source("./R/cmPlot_function.R") # required to plot CMs

x.train.red = as.data.frame(red_train[,1:11], ncol=11)
y.train.red = factor(red_train$quality)

x.test.red = as.data.frame(red_test[,1:11], ncol=11)
y.test.red = factor(red_test$quality)

x.train.white = as.data.frame(white_train[,1:11], ncol=11)
y.train.white = factor(white_train$quality)

x.test.white = as.data.frame(white_test[,1:11], ncol=11)
y.test.white = factor(white_test$quality)

# group into low (3-4), mid (5-6) and high (7-9)
y.train.red.grp <- y.train.red
levels(y.train.red.grp)[1:2] <- "low"
levels(y.train.red.grp)[2:3] <- "mid"
levels(y.train.red.grp)[3:4] <- "high"

y.test.red.grp <- y.test.red
levels(y.test.red.grp)[1:2] <- "low"
levels(y.test.red.grp)[2:3] <- "mid"
levels(y.test.red.grp)[3:4] <- "high"

y.train.white.grp <- y.train.white
levels(y.train.white.grp)[1:2] <- "low"
levels(y.train.white.grp)[2:3] <- "mid"
levels(y.train.white.grp)[3:5] <- "high"

y.test.white.grp <- y.test.white
levels(y.test.white.grp)[1:2] <- "low"
levels(y.test.white.grp)[2:3] <- "mid"
levels(y.test.white.grp)[3:5] <- "high"

#--------------------------------------------------------------------#

```

## Introduction

* Wine tasting can range from a casual pastime to a lucrative profession
* For professional sommeliers, considerable time and training is required to adequately rate wine quality 
* Intuitively, we might expect expert ratings to reflect the underlying chemical composition of the wines 
* We decided to more formally investigate this idea 

## Project Aim

To determine how accurately expert wine quality ratings can be predicted using a set of easily measured chemical components.

## Data

* 2 datasets of  expert quality ratings of red and white Vinho Verde wines 
  + source: http://archive.ics.uci.edu/ml/datasets/wine+quality
  + `r nrow(red_test) + nrow(red_train)` red wines 
  + `r nrow(white_test) + nrow(white_train)` white wines 
* Outcome variable: wine quality
    + ordinal variable theoretically ranging from 0-10
      - Observed ratings range from 3-9 
    + 0 is poor, 10 is excellent 
    + classes are unbalanced
* Predictor variables: 11 physiochemical wine components:
    + fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol

## Looking At the Red Wine Data
```{r red.boxplots, echo=FALSE, warning=FALSE}
# red wine
grid.arrange(
  ggplot(red_all, aes(x = quality, y = fixed_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = volatile_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = citric_acid)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = residual_sugar)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = chlorides)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = free_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = total_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = density)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = pH)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = sulphates)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = alcohol)) +
    geom_boxplot()+geom_jitter(alpha = .25), ncol=4
)
```

## Looking At the White Wine Data
```{r white.boxplots, echo=FALSE, warning=FALSE}
# white wine
grid.arrange(
  ggplot(white_all, aes(x = quality, y = fixed_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = volatile_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = citric_acid)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = residual_sugar)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = chlorides)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = free_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = total_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = density)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = pH)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = sulphates)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = alcohol)) +
    geom_boxplot()+geom_jitter(alpha = .25), ncol=4
)
```

## Correlations- Red Wine
```{r red.correlations, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.width='100%'}
# correlations and p-values by variable pairs
rcor_red <-rcorr(as.matrix(red_all[,1:11]))

# plot of correlations
corrplot(rcor_red$r, type="upper", order="hclust", tl.col = "black",
         p.mat = rcor_red$P, sig.level = 0.01, insig = "blank")
```

## Correlations- White Wine
```{r white.correlations, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.width='100%'}
# correlations and p-values by variable pairs
rcor_white <-rcorr(as.matrix(white_all[,1:11]))

# plot of correlations
corrplot(rcor_white$r, type="upper", order="hclust", tl.col = "black",
         p.mat = rcor_white$P, sig.level = 0.01, insig = "blank")
```

## Methods

* The red and white wine datasets were split into training and testing sets
  + 80% of available data sampled for training models
  + Remaining 20% used to evaluate accuracy
  + Relative frequencies of quality ratings in full dataset were preserved in training and testing data 
* Modeling Approach
  + Machine learning
    - Random forest classification of quality ratings 
  + Likelihood based 
    - linear regression 
    - partial proportional odds model 
    - multinomial model 

## Methods: Linear Regression 

* $Z = X^T \beta + \epsilon$, where $\epsilon\sim N(0,\sigma^2 I)$
* $\hat\beta = (X^T X)^{-1}X^T Z$
* Predicted classification: The nearest integer to the predicted mean rating 

## Methods: Partial Proportional Odds Models 

Three different approaches were considered:

* Non-proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x^T\beta_j$
* Proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x^T\beta$
* Partial proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x^T_*\beta + x^T_{**}\beta_j$
* $\alpha_j > \alpha_i$ for $j > i$
* Log likelihood maximized using BFGS (default) or any available method in package optimx
* Predicted classification: the rating with the highest predicted probability

## Methods: Multinomial Regression

* Response takes values in K classes
* Model: $\log \frac{Pr(G = k | X = x)}{P(G = K | X = x)} = X\beta_k$ for $k = 1,...,(K-1)$
* Predictions: $P(G = k | X = x) = \frac{\exp(X\beta_k)}{1 + \sum_{l = 1}^{K - 1}\exp(X\beta_l)},k = 1,..., K-1$
* Considered 3 Models: All Predictors with Linear Terms, Reduced Model with Linear Terms, and Reduced Model with All Second Order Terms
* Computational Efficiency Challenges

## Methods: Random Forest

* Ensembles of decision trees which are built by bootstrapping the observations
* Final prediction is made by a majority vote of the ensemble of trees


## Variable Selection

* Random Forest: all physiochemical variables included  
* Likelihood based models:
  + We examined the correlations between predictors and considered best subsets found in R based on linear regression.
  + The final predictor variables selected were:
    - red wine: volatile acidity, total sulfur dioxide, pH, alcohol, sulphates
    - white wine: pH, volatile_acidity, residual_sugar, alcohol
    
## Model Evaluation 

Models were compared on the following metrics:

* Accuracy
  + Correct Classifications/Total 
* Kappa 
* Weighted Kappa - used to select the final model
  + A more useful version of Kappa for data with inherent ordering that penalizes misclassifications proportional to the distance from the true category.
  + For instance, when the true quality rating is 4, a prediction of 7 will be penalized more severely than a prediction of 5. 
  
## Results: Linear Model 
```{r redlinplot, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
lllin = function(color) {
  if (color=='red') {
    beta = c('volatile.acidity', 'total.sulfur.dioxide','pH','alcohol','sulphates')
    test = red_test ; train = red_train
  } else if (color=='white') {
    beta = c('volatile.acidity', 'density','pH','alcohol','residual.sugar')
    test = white_test ; train = white_train
  }
  
  dat = train[,which(names(train)%in% c(beta,'quality'))]
  fit = linear.mod(dat)
  betahat = fit$betahat
  
  ypred = round(as.matrix(cbind(1,test[,which(names(test)%in% rownames(betahat)[-1])]),nrow=nrow(test)) %*% betahat)
  result = data.frame(real = test$quality, pred = round(ypred))
  return(result)
}

# confusion matrix
linredresult = linrfac = lllin('red') ; linwhiteresult = linwfac = lllin('white') 
linrfac$real = factor(linredresult$real)
linrfac$pred = factor(linredresult$pred, levels=levels(linrfac$real))
linwfac$real = factor(linwhiteresult$real)
linwfac$pred = factor(linwhiteresult$pred, levels=levels(linwfac$real))

linredCM = confusionMatrix(data=linrfac$pred, reference=linrfac$real)
linwhiteCM = confusionMatrix(data=linwfac$pred, reference=linwfac$real)

# classification results (into quality score)
#cM.red$table
linrclass <- as.data.frame(linredCM$table)
linrclass$Freq[linrclass$Freq == 0] <-NA

#cM.white$table
linwclass <- as.data.frame(linwhiteCM$table)
linwclass$Freq[linwclass$Freq == 0] <-NA

# For TABLE
# prediction accuracy by category
linrpred <- round(diag(linredCM$table)/table(linrfac$real), 4)*100
linwpred <- round(diag(linwhiteCM$table)/table(linwfac$real), 4)*100

# prediction accuracy and kappa
linracc = round(linredCM$overall['Accuracy'], 4) * 100
linwacc = round(linwhiteCM$overall['Accuracy'], 4) * 100

# weighted kappas
wklin = wkappa(linwhiteCM$table)
rklin = wkappa(linredCM$table)

```

* Red Wine
  + Accuracy: `r linracc`
  + Kappa: `r round(rklin$kappa,4)`
  + Weighted Kappa: `r round(rklin$weighted.kappa,4)`
  
* White Wine
  + Accuracy: `r linwacc`
  + Kappa: `r round(wklin$kappa,4)`
  + Weighted Kappa: `r round(wklin$weighted.kappa,4)`

## Results: Linear Model (Red Wine)
```{r, echo=F}
# classification results as a plot
cmPlot(linrclass, "red", pred_first = TRUE,
       "Linear Model Classification of Quality for Red Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Linear Model (White Wine)
```{r whitelinplot, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
cmPlot(linwclass, "white", pred_first = TRUE,
      "Linear Model Classification of Quality for White Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Partial Proportional Odds Model (White Wine)

```{r, echo=F, warning=F, message = F, results = 'hide', eval=F}
beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + residual.sugar,
                       data = white_train))

#fully proportional odds 
white.prop.odds <- partial.prop.odds.mod(y ="quality", in.data = white_train,
                              prop.odds.formula = ~ alcohol+ pH + volatile.acidity + residual.sugar,
                              beta.prop.odds.start = beta.starts[c(2:5)],
                              method = "BFGS")
saveRDS(white.prop.odds, "proportional_odds_models/white.prop.odds.rds")


```

```{r, echo=F, warning=F, message = F, results = 'hide'}
#load saved models 
white.prop.odds <- readRDS("proportional_odds_models/white.prop.odds.rds")

#predict values 
white.preds <- predict(white.prop.odds, white_test)$most.likely

#compare to actual values  
white.pred.table <- data.frame(table(factor(white.preds, levels = sort(unique(white.obs))), white.obs))
white.level.accuracy <- sapply(sort(unique(white.obs)), function(x){
  
  target <- white.pred.table[ white.pred.table$white.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$white.obs])
  100*correct/total
  
})
white.pred.table$Freq[white.pred.table$Freq == 0] <- NA
white.pred.accuracy <- 100*sum(white.preds == white.obs)/length(white.obs)
white.pred.kappa <- cohen.kappa(cbind(white.preds, white.obs))$kappa
white.pred.wkappa <- cohen.kappa(cbind(white.preds, white.obs))$weighted.kappa
white.prop.odds.metrics <- c(white.pred.accuracy, white.pred.kappa, white.pred.wkappa,
                              white.level.accuracy)
```

* For white wine, only the proportional odds model converged 
  + Overall Accuracy: `r round(white.pred.accuracy, 2)`%
  + Kappa: `r round(white.pred.kappa, 4)`
  + Weighted Kappa: `r round(white.pred.wkappa, 4)`

## Results: Proportional Odds Model (White Wine)

```{r, echo=F, warning=F, message=F}
#plotting confusion matrix 
cmPlot(white.pred.table, "white", pred_first = T, title = "Proportional Odds Model Predicted vs Observed", axis.title.size = 20, axis.text.size = 25)

```

## Results: Partial Proportional Odds Model (Red Wine)

* Proportional, partial proportional, and and non-proportional odds models all converged
* Results presented for partial proportional model with coefficient for total sulfur dioxide allowed to vary with the level of wine quality

```{r,echo=F, warning=F, message = F, results='hide', eval=F}

beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + sulphates + total.sulfur.dioxide,
                       data = red_train))

#partial proportional odds model
red.partial.prop <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol + pH+ volatile.acidity + sulphates,
                              beta.prop.odds.start = beta.starts[2:5],
                              non.prop.odds.formula = ~total.sulfur.dioxide,
                              beta.non.prop.odds.start = matrix(rep(beta.starts[6], 5), nrow = 1),
                              method = "BFGS")

#proportional odds model
red.prop.odds <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol + pH + volatile.acidity + sulphates
                              + total.sulfur.dioxide,
                              beta.prop.odds.start = beta.starts[2:6],
                              method = "BFGS")

#all non-proportional 
red.non.prop <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              non.prop.odds.formula = ~ alcohol + pH+ volatile.acidity + sulphates
                              + total.sulfur.dioxide,
                              beta.non.prop.odds.start = matrix(rep(beta.starts[2:6], each = 5), nrow = 5, 
                                                                byrow = T),
                              method = "BFGS")
saveRDS(red.partial.prop, "proportional_odds_models/red.partial.prop.rds")
saveRDS(red.prop.odds, "proportional_odds_models/red.prop.odds.rds")
saveRDS(red.non.prop, "proportional_odds_models/red.non.prop.rds")

```

```{r,echo=F, warning=F, message = F, results='hide'}

#load saved models 
red.partial.prop <- readRDS("proportional_odds_models/red.partial.prop.rds")
red.prop.odds <- readRDS("proportional_odds_models/red.prop.odds.rds")
red.non.prop <- readRDS("proportional_odds_models/red.non.prop.rds")

#partial proportional odds model predictions/metrics
red.partial.prop.preds <- predict(red.partial.prop, red_test)$most.likely
red.partial.prop.preds.table <- data.frame(table(factor(red.partial.prop.preds, levels = sort(unique(red.obs))), red.obs))
red.partial.prop.level.accuracy <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.partial.prop.preds.table[ red.partial.prop.preds.table$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.partial.prop.preds.table$Freq[red.partial.prop.preds.table$Freq == 0] <- NA
red.partial.prop.accuracy <- 100*sum(red.partial.prop.preds == red.obs)/length(red.obs)
red.partial.prop.kappa <- cohen.kappa(cbind(red.partial.prop.preds, red.obs))$kappa
red.partial.prop.wkappa <- cohen.kappa(cbind(red.partial.prop.preds, red.obs))$weighted.kappa
red.partial.prop.metrics <- c(red.partial.prop.accuracy, red.partial.prop.kappa, red.partial.prop.wkappa,
                              red.partial.prop.level.accuracy)

#proportional odds model predictions/metrics
red.prop.odds.preds <- predict(red.prop.odds, red_test)$most.likely

red.prop.odds.table <- data.frame(table(factor(red.prop.odds.preds, levels = sort(unique(red.obs))), red.obs))
red.prop.odds.level.accuracy <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.prop.odds.table[ red.prop.odds.table$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.prop.odds.table$Freq[red.prop.odds.table$Freq == 0] <- NA
red.prop.odds.accuracy <- 100*sum(red.prop.odds.preds == red.obs)/length(red.obs)
red.prop.odds.kappa <- cohen.kappa(cbind(red.prop.odds.preds, red.obs))$kappa
red.prop.odds.wkappa <- cohen.kappa(cbind(red.prop.odds.preds, red.obs))$weighted.kappa
red.prop.odds.metrics <- c(red.prop.odds.accuracy, red.prop.odds.kappa, red.prop.odds.wkappa, red.prop.odds.level.accuracy)

#non-proportional odds predictions/metrics
red.non.prop.preds <- predict(red.non.prop, red_test)$most.likely
red.non.prop.accuracy <- 100*sum(red.non.prop.preds == red.obs)/length(red.obs)
red.non.prop.kappa <- cohen.kappa(cbind(red.non.prop.preds, red.obs))$kappa
red.non.prop.wkappa <- cohen.kappa(cbind(red.non.prop.preds, red.obs))$weighted.kappa
red.non.prop.odds.table <- data.frame(table(factor(red.non.prop.preds, levels = sort(unique(red.obs))), red.obs))
red.non.prop.level.accuracy <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.non.prop.odds.table[ red.non.prop.odds.table$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.non.prop.odds.table$Freq[red.non.prop.odds.table$Freq == 0] <- NA
red.non.prop.odds.metrics <- c(red.non.prop.accuracy, red.non.prop.kappa, red.non.prop.wkappa, red.non.prop.level.accuracy)

#comparison of methods 
red.partial.prop.model.comp <- data.frame(Model = c("Proportional Odds", "Partial Proportional Odds",
                                                    "Non-Proportional Odds"), 
                                          Accuracy = c(red.prop.odds.accuracy, red.partial.prop.accuracy, 
                                                       red.non.prop.accuracy), 
                                          Kappa = c(red.prop.odds.kappa, red.partial.prop.kappa, 
                                                       red.non.prop.kappa),
                                          Weighted.Kappa = c(red.prop.odds.wkappa, red.partial.prop.wkappa, 
                                                       red.non.prop.wkappa)) 
colnames(red.partial.prop.model.comp) <- c("Model", "Accuracy", "Kappa", "Weighted Kappa")
```

## Results: Comparison of Partial Proportional Odds Models (Red Wine)

```{r, echo=F}
kable(red.partial.prop.model.comp, caption = "Comparison of Partial Proportional Odds Models for Red Wine Quality",
      digits = 4, booktabs=T)
```

## Results: Proportional Odds Model (Red Wine)

```{r, echo=F, warning=F, message = F}
#plotting confusion matrix 
cmPlot(red.prop.odds.table, "red", pred_first = T, title = "Proportional Odds Predicted vs Observed", axis.title.size = 20, axis.text.size = 25)
```

## Results: Partial Proportional Odds Model (Red Wine)

```{r, echo=F, warning=F, message = F}
cmPlot(red.partial.prop.preds.table, "red", pred_first = T, title = "Partial Proportional Odds Predicted vs Observed", axis.title.size = 20, axis.text.size = 25)
```

## Results: Non-Proportional Odds Model (Red Wine)

```{r, echo=F, warning=F, message = F}
cmPlot(red.non.prop.odds.table, "red", pred_first = T, title = "Non-Proportional Odds Predicted vs Observed", axis.title.size = 20, axis.text.size = 25)
```

```{r eval=FALSE, message=F, include=FALSE, results='hide'}
#Begin Multinomial Results Section

set.seed(13)
red_train_full_fit <- fit_multinomial_regression(red_train, quality ~ 1 + ., ref_level = "8", trace = 1, itters = 300)

red_train_reduced_linear_fit <- fit_multinomial_regression(red_train, quality ~ 1 + alcohol + volatile.acidity + total.sulfur.dioxide + pH + sulphates, ref_level = "8", trace = 1, itters = 300)

red_train_reduced_all_quad_fit <- fit_multinomial_regression(red_train, quality ~ 1 + alcohol + volatile.acidity + total.sulfur.dioxide + pH+ I(alcohol*alcohol) + I(volatile.acidity*volatile.acidity) + I(pH*pH) + I(total.sulfur.dioxide*total.sulfur.dioxide) + I(sulphates*sulphates) + (alcohol + volatile.acidity + total.sulfur.dioxide + pH + sulphates)^2, ref_level = "8", trace = 1, itters = 300)

white_train_full_fit <- fit_multinomial_regression(white_train, quality ~ 1 + ., ref_level = "8", trace = 1, itters = 300)

white_train_linear_fit <- fit_multinomial_regression(white_train, quality ~ 1 + pH + volatile.acidity + residual.sugar + alcohol, ref_level = "8", trace = 1, itters = 300)

#Takes a long long long time to run
white_train_quad_fit <- fit_multinomial_regression(white_train, quality ~ 1 + pH + volatile.acidity + residual.sugar + alcohol + (pH + volatile.acidity + residual.sugar + alcohol)^2 + I(pH*pH) + I(volatile.acidity*volatile.acidity) + I(residual.sugar*residual.sugar) + I(alcohol*alcohol), ref_level = "8", trace = 1, itters = 300)


saveRDS(red_train_full_fit, "multinomial_models/red_full_fit.rds")
saveRDS(red_train_reduced_linear_fit, "multinomial_models/red_reduced_linear_fit.rds")
saveRDS(red_train_reduced_all_quad_fit, "multinomial_models/red_reduced_all_quad_fit.rds")
saveRDS(white_train_full_fit, "multinomial_models/white_full_fit.rds")
saveRDS(white_train_linear_fit, "multinomial_models/white_reduced_linear_fit.rds")
saveRDS(white_train_quad_fit, "multinomial_models/white_reduced_quad_fit.rds")
```

```{r include=FALSE, message=F, warning=F}
red_train_full_fit <- readRDS("multinomial_models/red_full_fit.rds")
red_train_reduced_linear_fit <- readRDS("multinomial_models/red_reduced_linear_fit.rds")
red_train_reduced_all_quad_fit <- readRDS("multinomial_models/red_reduced_all_quad_fit.rds")
white_train_full_fit <- readRDS("multinomial_models/white_full_fit.rds")
white_train_linear_fit <- readRDS("multinomial_models/white_reduced_linear_fit.rds")
white_train_quad_fit <- readRDS("multinomial_models/white_reduced_quad_fit.rds")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Fit predictions
red_test_full_preds <- predict_multinomial(red_train_full_fit, red_test)
red_test_reduced_linear_preds <- predict_multinomial(red_train_reduced_linear_fit, red_test)
red_test_reduced_quad_preds <- predict_multinomial(red_train_reduced_all_quad_fit, red_test)
white_test_full_preds <- predict_multinomial(white_train_full_fit, white_test)
white_test_linear_preds <- predict_multinomial(white_train_linear_fit,
                                              white_test)
white_test_quad_preds <- predict_multinomial(white_train_quad_fit, white_test)
```

## Results: Multinomial Regression (Red Wine Quality Classification)
```{r message=FALSE, warning=FALSE, echo=F}
multinomial_red_results <- bind_rows(
  pred_summary_stats(red_test_full_preds, red_test$quality, "Full Model (Linear Terms)"),
  pred_summary_stats(red_test_reduced_linear_preds, red_test$quality, "Reduced Model (Linear Terms)"),
   pred_summary_stats(red_test_reduced_quad_preds, red_test$quality, "Reduced Model (Second Order Terms)")
) %>% dplyr::arrange(desc(`Weighted Kappa`)) 
colnames(multinomial_red_results)[2] <- "Accuracy"

multinomial_red_results %>% 
  kable(caption = "Comparison of Multinomial Regression Models for Red Wine Quality", 
        booktabs = T, digits = 4) %>%
  kable_styling(latex_options = c("scale_down"))

#Remove "model" because it's not in the big table. 
multinomial_best_red_model_results <- multinomial_red_results %>% slice(1) %>% 
  select(-Model)
```

## Results: Multinomial Regression - Full Model Confusion Matrix (Red Wine)
```{r, echo=F, warning=F, message = F, echo=F}
multinom_red_best_cm <- confusionMatrix(as.factor(red_test_full_preds), as.factor(red_test$quality))$table %>% 
  as.data.frame()

multinom_red_best_cm$Freq[multinom_red_best_cm$Freq == 0] <-NA
cmPlot(multinom_red_best_cm, "red", pred_first = T, title = "Multinomial Regression Confused\n on Non-Average Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Multinomial Regression (White Wine Quality Classification)
```{r message=FALSE, warning=FALSE,echo=F}
multinomial_white_results <- bind_rows(
  pred_summary_stats(white_test_full_preds, white_test$quality, "Full Model (Linear Terms)"),
  pred_summary_stats(white_test_linear_preds, white_test$quality, "Reduced Model (Linear Terms)"),
   pred_summary_stats(white_test_quad_preds, white_test$quality, "Reduced Model (Second Order Terms)"))%>% 
  dplyr::arrange(desc(`Weighted Kappa`))
colnames(multinomial_white_results)[2] <- "Accuracy"

multinomial_white_results %>% 
  kable(caption = "Comparison of Multinomial Regression Models for Red Wine Quality", 
        booktabs = T, digits = 4) %>%
        kable_styling(latex_options = c("scale_down"))

multinomial_white_best_model_results <- multinomial_white_results %>%
  slice(1) %>% 
  select(-Model)
```

## Results: Multinomial Regression - Full Model Confusion Matrix (White Wine)
```{r, echo=F, warning=F, message = F}
multinom_white_best_cm <- confusionMatrix(as.factor(white_test_full_preds), as.factor(white_test$quality))$table %>% 
  as.data.frame()

multinom_white_best_cm$Freq[multinom_white_best_cm$Freq == 0] <-NA
cmPlot(multinom_white_best_cm, "white", pred_first = T, title = "Multinomial Regression Confused\n on Non-Average Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Random Forest
```{r red.rf, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
#source("./R/cmPlot_function.R") # required to plot CMs
# train random forest
trCtl <- trainControl(savePredictions=TRUE)
#fit.red <- train(x.train.red, y.train.red, method="rf", trControl=trCtl)
#fit.white <- train(x.train.white, y.train.white, method="rf", trControl=trCtl)
# saveRDS(fit.red, "rf_models/rf_red_fit.rds")
# saveRDS(fit.white, "rf_models/rf_white_fit.rds")
fit.red <- readRDS("rf_models/rf_red_fit.rds")
fit.white <- readRDS("rf_models/rf_white_fit.rds")

# random forest training results
#fit.red$results
#fit.white$results

# use training model to predict y
y.pred.red <- predict(fit.red, x.test.red)
y.pred.white <- predict(fit.white, x.test.white)

# results of rf on test data
cM.red = confusionMatrix(data=y.pred.red, reference=y.test.red)
cM.white = confusionMatrix(data=y.pred.white, reference=y.test.white)

# classification results (into quality score)
#cM.red$table
class.red <- as.data.frame(cM.red$table)
class.red$Freq[class.red$Freq == 0] <-NA

#cM.white$table
class.white <- as.data.frame(cM.white$table)
class.white$Freq[class.white$Freq == 0] <-NA

# prediction accuracy by category
pred.red <- round(diag(cM.red$table)/table(y.test.red), 4)*100
pred.white <- round(diag(cM.white$table)/table(y.test.white), 4)*100

# prediction accuracy and kappa
res.red = round(cM.red$overall[1:2], 4)
res.red[1] = res.red[1]*100
res.white = round(cM.white$overall[1:2], 4) 
res.white[1] = res.white[1]*100

# weighted kappas
wk.red <- round(cohen.kappa(cM.red$table)$weighted.kappa, 4)
wk.white <- round(cohen.kappa(cM.white$table)$weighted.kappa, 4)

# print results
rf.results <- setNames(data.frame(matrix(ncol = 10, nrow = 2)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8", "9"))
rownames(rf.results) <- c("Red Wine", "White Wine")

pred.red2 <- c(pred.red, NA)
rf.results[1,] <- c(res.red, wk.red, pred.red2)
rf.results[2,] <- c(res.white, wk.white, pred.white)

kable(rf.results, caption = "Random Forest Results for Red and White Wine", booktabs=T, digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Results: Random Forests (Red Wine)

```{r, echo=F}
# classification results as a plot
cmPlot(class.red, "red", pred_first = TRUE,
"RF Classification of Quality for Red Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Random Forests (White Wine)

```{r white.rf, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
cmPlot(class.white, "white", pred_first = TRUE,
"RF Classification of Quality for White Wines", axis.title.size = 20, axis.text.size = 25)
```

## Comparison of Results: Red Wine
```{r echo=FALSE, message=FALSE, warning=FALSE}
#-----------------------------------------------------------------------------#
#-----------------------------------------------------------------------------#
# Create Results Data Tables
# round results to 4 digits
# accuracy into categories as percent
# rename row and fill in data accordingly (add more rows if needed)

#-----------------------------------------------------------------------------#
# RED WINE NO GROUPING
results.red <- setNames(data.frame(matrix(ncol = 9, nrow = 6)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8"))
rownames(results.red) <- c("Random Forest", "Proportional Odds", "Partial Proportional Odds",
                           "Non-Proportional Odds", "Multinomial", "Linear Regression")

results.red[1,] <- c(res.red, wk.red, pred.red)
results.red[2, ] <- red.prop.odds.metrics
results.red[3, ] <- red.partial.prop.metrics
results.red[4, ] <- red.non.prop.odds.metrics
results.red["Multinomial", ] <- multinomial_best_red_model_results
results.red['Linear Regression', ] <- c(linracc, round(rklin$kappa,4), round(rklin$weighted.kappa,4),linrpred)


#-----------------------------------------------------------------------------#
# WHITE WINE NO GROUPING
results.white <- setNames(data.frame(matrix(ncol = 10, nrow = 4)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8", "9"))
rownames(results.white) <- c("Random Forest", "Proportional Odds", "Multinomial", "Linear Regression")

results.white[1,] <- c(res.white, wk.white, pred.white)
results.white[2, ] <- white.prop.odds.metrics 
results.white["Multinomial", ] <- multinomial_white_best_model_results
results.white['Linear Regression', ] <- c(linwacc, round(wklin$kappa,4), round(wklin$weighted.kappa,4),linwpred)

#-----------------------------------------------------------------------------#
# Print out comparison tables created above
# Red wine no groups
kable(results.red, caption = "Comparison of Results for Red Wine", booktabs=T,digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 6)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))

#-----------------------------------------------------------------------------#
```

## Comparison of Results: White Wine
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Print out comparison tables created above

# White wine no groups
kable(results.white, caption = "Comparison of Results for White Wine", booktabs=T, digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Discussion: Random Forest    

* Using random forests, we were able to predict expert wine quality ratings fairly accurately using measurements of a small number of chemical components of wine
  + White wine
    - Accuracy: `r res.white[1]`% 
    - Weighted Kappa: `r wk.white`
  + Red wine
    - Accuracy: `r res.red[1]`% 
    - Weighted Kappa: `r wk.red`
* Using the proposed cutoffs from Landis and Koch, these weighted Kappa values suggest moderate to substantial agreement with the expert ratings

## Discussion: Likelihood Based Approaches

* The likelihood based approaches performed more poorly, with accuracies 10-15% lower than random forest and weighted kappas 0.1 to 0.2 lower
* Even so, the weighted kappas all suggested moderate agreement with the expert ratings and were clearly better than random guessing
* There was not a clear winner among the likelihood based approaches, accuracies and kappas were very similar across multinomial, ordinal, and linear regression models
  + However, linear model showed a tendency to predict as a mean value, which resulted in poor prediction other than the middle quality categories
  + Accounting for the ordered nature of the ratings did not make a difference in predictions 
  + In some cases, doing so actually resulted in (slightly) worse predictive performance
  
## Discussion: Limitations and Future Directions 

* We only had data on a small number of reasonably easy to measure chemicals, we speculate some misclassification was due to failure to measure other important chemicals.
* Our goal was simply to predict quality ratings, in the future, it would be interesting characterize how individual chemicals relate to wine quality. 
* There were only a small number of truly excellent wines (rating of 8 or higher) making it particularly difficult to predict these ratings. It would be worthwhile to examine whether various outlier detection algorithms are more suited to characterizing these wines than the approaches we took. 

## Bottom Line

Expert wine quality ratings can be predicted reasonably well using chemical components, but true wine connoisseurs are still better off consulting a sommelier. 

## References

Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. 1977 Mar;33(1):159-74.



