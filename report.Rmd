---
title: "sommelieR"
author: "Seoyoon Cho, Paloma Hauser, Taylor Lagler, Mike Nodzenski, Bryce Rowland"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
---

```{r pressure, echo=FALSE, out.width = '25%', fig.align="center"}
knitr::include_graphics("images/sommelieR.jpg")
```

## Introduction

  Wine tasting can range from a casual pastime to a lucrative profession. For professional sommeliers, considerable time and training is required to adequately rate wine quality. Intuitively, we expect expert ratings to reflect the underlying chemical composition of the wines. To this end, we hypothesized it would be possible to predict ratings using statistical models based on the chemical makeup of wines. 
  
## Project Aim

To determine how accurately expert wine quality ratings can be predicted using a set of easily measured chemical components.

```{r loadpackage, include = F}
# create the datasets for training and testing for the group project
set.seed(13)

library(caret)
library(readr)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(Hmisc)
library(corrplot)
library(tidyverse)
library(kableExtra)
library(knitr)
library(devtools) 
library(psych)
#install sommelieR
install_github("group-wine/sommelieR")
library(sommelieR)

#load the datasets 
data("red_test")
data("red_train")
data("white_test")
data("white_train")
```

## Data

Two datasets of expert quality ratings of red and white Vinho Verde wines were used. The data is obtained from  http://archive.ics.uci.edu/ml/datasets/wine+quality. There are total of `r nrow(red_test) + nrow(red_train)` red wines and `r nrow(white_test) + nrow(white_train)` white wines in the two datasets.

The outcome variable is wine quality. This variable is an ordinal variable theoretically ranging from 0-10. However, the observed ratings only range from 3-9, where 0 indicates poor quality and 10 is for excellent quality. The data is highly unbalanced across quality classes.

Predictor variables are 11 physiochemical wine components: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.

Below are the boxplots showing how each predictors are distributed across quality for red and white wine datasets.


```{r setup, include = F}
white.obs <- white_test$quality
red.obs <- red_test$quality

#center and scale the predictors 
red_test[ , colnames(red_test) != "quality"] <- apply(red_test[ , colnames(red_test) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
red_train[ , colnames(red_train) != "quality"] <- apply(red_train[ , colnames(red_train) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
white_test[ , colnames(white_test) != "quality"] <- apply(white_test[ , colnames(white_test) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))
white_train[ , colnames(white_train) != "quality"] <- apply(white_train[ , colnames(white_train) != "quality"], 
                                                      2, function(x) (x - mean(x))/sd(x))

#--------------------------------------------------------------------#
# Data set up for boxplot code:
red_all = rbind(red_test, red_train)
red_all$quality <- as.factor(red_all$quality)
names(red_all) = gsub("\\.", "_", names(red_all))

white_all = rbind(white_test, white_train)
white_all$quality <- as.factor(white_all$quality)
names(white_all) = gsub("\\.", "_", names(white_all))

#--------------------------------------------------------------------#
# Data set up for RF classification by type code:

# merge data sets, add type indicator
type <- c(rep("red", dim(red_train)[1]),rep("white", dim(white_train)[1]))
all_train_a <- rbind(red_train, white_train)
all_train <- cbind(all_train_a, type)

type2 <- c(rep("red", dim(red_test)[1]),rep("white", dim(white_test)[1]))
all_test_a <- rbind(red_test, white_test)
all_test <- cbind(all_test_a, type2)

# convert type and quality to factor
all_train$type <- as.factor(all_train$type)
all_train$quality <- as.factor(all_train$quality)

all_test$type <- as.factor(all_test$type)
all_test$quality <- as.factor(all_test$quality)


x.train = as.data.frame(all_train[,1:11], ncol=11)
y.train = factor(all_train$type)

x.test = as.data.frame(all_test[,1:11], ncol=11)
y.test = factor(all_test$type)

#--------------------------------------------------------------------#
# Data set up for RF classification by quality code:
#source("./R/cmPlot_function.R") # required to plot CMs

x.train.red = as.data.frame(red_train[,1:11], ncol=11)
y.train.red = factor(red_train$quality)

x.test.red = as.data.frame(red_test[,1:11], ncol=11)
y.test.red = factor(red_test$quality)

x.train.white = as.data.frame(white_train[,1:11], ncol=11)
y.train.white = factor(white_train$quality)

x.test.white = as.data.frame(white_test[,1:11], ncol=11)
y.test.white = factor(white_test$quality)

# group into low (3-4), mid (5-6) and high (7-9)
y.train.red.grp <- y.train.red
levels(y.train.red.grp)[1:2] <- "low"
levels(y.train.red.grp)[2:3] <- "mid"
levels(y.train.red.grp)[3:4] <- "high"

y.test.red.grp <- y.test.red
levels(y.test.red.grp)[1:2] <- "low"
levels(y.test.red.grp)[2:3] <- "mid"
levels(y.test.red.grp)[3:4] <- "high"

y.train.white.grp <- y.train.white
levels(y.train.white.grp)[1:2] <- "low"
levels(y.train.white.grp)[2:3] <- "mid"
levels(y.train.white.grp)[3:5] <- "high"

y.test.white.grp <- y.test.white
levels(y.test.white.grp)[1:2] <- "low"
levels(y.test.white.grp)[2:3] <- "mid"
levels(y.test.white.grp)[3:5] <- "high"

#--------------------------------------------------------------------#

```

```{r useful_functions, include = F}
pred_summary_stats <- function(preds, true_values, model){
  confusion_matrix_obj <- confusionMatrix(as.factor(preds),
                                      as.factor(true_values))
  accuracy <- confusion_matrix_obj$overall[1]
  kappa <- confusion_matrix_obj$overall[2]
  wt.kappa <- cohen.kappa(confusion_matrix_obj$table)$weighted.kappa
  
  
  pct_correct_by_cat <- (diag(confusion_matrix_obj$table)/table(true_values))
  pct_correct_df <- as.data.frame(pct_correct_by_cat) %>% 
    spread(true_values, Freq)
  pred_sum_stats <- bind_cols("Model" = model,"Prediction Accuracy" = accuracy,Kappa = kappa, `Weighted Kappa` = wt.kappa, pct_correct_df) %>% 
    mutate_at(.vars = vars(-c("Model", "Kappa", "Weighted Kappa")),
              .funs = function(x){x*100})
  return(pred_sum_stats)
}

```

<!--## Looking At the Red Wine Data-->
```{r red.boxplots, echo=FALSE, warning=FALSE, fig.width=9, fig.height=6, fig.align='center'}
# red wine
grid.arrange(
  ggplot(red_all, aes(x = quality, y = fixed_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = volatile_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = citric_acid)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = residual_sugar)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = chlorides)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = free_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = total_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = density)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = pH)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = sulphates)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(red_all, aes(x = quality, y = alcohol)) +
    geom_boxplot()+geom_jitter(alpha = .25), ncol=4
)
```

<!--## Looking At the White Wine Data-->
```{r white.boxplots, echo=FALSE, warning=FALSE, fig.width=9, fig.height=6, fig.align='center'}
# white wine
grid.arrange(
  ggplot(white_all, aes(x = quality, y = fixed_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = volatile_acidity)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = citric_acid)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = residual_sugar)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = chlorides)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = free_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = total_sulfur_dioxide)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = density)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = pH)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = sulphates)) +
    geom_boxplot()+geom_jitter(alpha = .25),
  ggplot(white_all, aes(x = quality, y = alcohol)) +
    geom_boxplot()+geom_jitter(alpha = .25), ncol=4
)
```

Below plots show the correlation among the 11 variables for each red and white wine datasets, respectively. Here, the darker the blue, the more positively the variables are correlated and the darker the red, the more negatively the variables are correlated.

<!--## Correlations-->
```{r correlations, echo=FALSE, fig.align='left', message=FALSE, warning=FALSE, out.width='80%'}
par(mfrow=c(1,2))
# correlations and p-values by variable pairs
rcor_red <-rcorr(as.matrix(red_all[,1:11]))

# plot of correlations
corrplot(rcor_red$r, type="upper", order="hclust", tl.col = "black",
         p.mat = rcor_red$P, sig.level = 0.01, insig = "blank")

# correlations and p-values by variable pairs
rcor_white <-rcorr(as.matrix(white_all[,1:11]))

# plot of correlations
corrplot(rcor_white$r, type="upper", order="hclust", tl.col = "black",
         p.mat = rcor_white$P, sig.level = 0.01, insig = "blank")
```

## Methods

#### Data processing

For the analysis, the red and white wine datasets were split into training and testing sets. The training data was sampled as 80% of the total available data and the remaining 20% was used as a testing data. We had training and test datasets for each red and white wine. The training data was used to train the model and the test data was used to evaluate the prediction accuracy of the model. Since the quality variable was highly unbalanced in the full dataset, the relative frequencies of this variable were preserved in both training and test data.

#### Modeling Approach 

We modeled wine quality ratings using the random forest machine learning method and three likelihood based models: linear, partial proportional odds, and multinomial regression. The details of this models are given below:

#### Linear Regression

Since the observed wine ratings were integers, it was unclear whether rating should be considered a continuous or ordinal random variable. As an initial step, we assumed the ratings were continuous and fit an OLS linear regression model as follows:

$$Z = X^T \beta + \epsilon,\ \text{where}\  \epsilon\sim N(0,\sigma^2 I)$$
where $Z$ is the response variable, quality, $X$ is the design matrix with predictor variables, $\beta$ are the regression coefficients, and we assume the response follows normal distribution with the distribution assumption for $\epsilon$. It is well known that the least squares estimate of $\beta$ is
$$\hat\beta = (X^T X)^{-1}X^T Z.$$
Since the predicted value of a linear model is continuous but the quality ratings were integers, we rounded the predicted linear regression values to the nearest integer to obtain the final predicted quality rating.

#### Partial Proportional Odds Models 

  To account for the ordinal nature of wine ratings without assuming continuity, we implemented three cumulative logit models: proportional odds, partial proportional odds, and non-proportional odds. Letting $Z$ correspond to level of wine rating , $i$ correspond to a specific wine, and $x_i$ correspond to the chemical components of wine i, the models are given below:     

* Non-proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x_i^T\beta_j$
* Proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x_i^T\beta$
* Partial proportional odds: $logit(P(Z \leq j|x)) = \alpha_j + x^T_{i*}\beta + x^T_{i**}\beta_j$

Where $\alpha_j > \alpha_i$ for $j > i$, and $x^T_{i*}$ and $x^T_{i**}$ are submatrices of $x_i$.

  The log likelihood for each model was maximized using the BFGS algorithm as implemented in the R package optimx (alternatively, users of our R package could specify any available method implemented in optimx). For models that converged, the predicted classification was taken to be the rating with the highest predicted probability. An S3 predict method was implemented in sommelieR to facilitate these predictions. 

#### Multinomial Regression

  We were also interested in testing if we could relax the ordinality assumption of our data and still retain predictive accuracy. Therefore, we decided to fit a multinomial regression model to our data as well.
  Consider a random variable, Y, which takes on values in 1,...,K classes, with covariates X. The multinomial regression model has the form,
  
  $$\log \frac{P(Y = k | X = x)}{P(Y = K | X = x)} = X\beta_k,\ \ k = 1,...,(K-1)$$

where $\beta_k$ is a $p \times 1$ vector of regression coefficient corresponding to level, k. Additionally, we apply the constraint that the probabilities sum to one, i.e. $\sum_{i = 1}^K P(Y = i | X = x) = 1$.
  To get the probabilties we used for preduction, we solve the above system of equations given the above constraint:
  
  $$P(Y = k | X = x) = \frac{\exp(X\beta_k)}{1 + \sum_{l = 1}^{K - 1}\exp(X\beta_l)},k = 1,..., K-1 \\ 
  P(Y = K | X = x) = \frac{1}{1 + \sum_{l = 1}^{K - 1}\exp(X\beta_l)}$$

This notation was taken from Elements of Statistical Learning, 2009 (Hastie, Tibshirani, and Friedman). 
  For both red and white wine we fit three multinomial regression models:
  
* All Predictors with Linear Terms 
* Reduced Model with Linear Terms
* Reduced Model with All Second Order Terms

where the coefficients in the reduced model - and how they were selected - are mentioned below. 
  While we would have liked to fit models with higher order terms (quadratic terms, interactions), with our current implementation of multinomial regression - this was impractical. For a response variable taking on K classes, each new variable added to the model adds (K-1) coefficients, which made fitting a full second order model challenging without binning our response variable. In the future, this challenge could be overcome through programming in C++ or using a penalization for each row of the coefficent matrix (since you could not simply penalize each individual coefficient). 

#### Random Forest

For a machine learning approach, we fit random forest. Using the standard implementation in the *caret* package, random forest constructs emsembles of decision trees which are built by bootstrapping the observations. The final prediction/classification is then made by a majority vote of the ensemble of trees.

#### Variable Selection

For the random forest models, we used all 11 physiochemical variables as predictors. For the likelihood based models, we considered full and reduced models. The full models for both red and white wines included all 11 physiochemical variables. The variables in the reduced models were determined by looking at the correlations between predictors and using best subsets based on OLS regression. The covariates selected for the red model were volatile acidity, total sulfur dioxide, pH, alcohol, and sulphates. The covariates selected for the white model were pH, volatile_acidity, residual_sugar, and alcohol. 

#### Model Evaluation 

We compared the four models on overall accuracy (correct predictions/total), kappa, and weighted kappa. Kappa is a commonly used statistic for capturing how well the classification is done compared to a 50% random chance classification. Weighted kappa is an extension of kappa that is a more useful for data with inherent ordering since it penalizes misclassifications proportional to the distance from the true category. For instance, when the true quality rating is 4, a prediction of 7 will be penalized more severely than a prediction of 5. Since our outcome variable, quality, is ordered, we used the weighted Kappa to select the final model.
  
## Results: Linear Model 

The predictive performance of the full and reduced linear regression models on the testing data are given below: 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
lllin = function(color,full=TRUE) {
  if (color=='red') {
    beta = c('volatile.acidity', 'total.sulfur.dioxide','pH','alcohol','sulphates')
    test = red_test ; train = red_train
  } else if (color=='white') {
    beta = c('volatile.acidity', 'pH','alcohol','residual.sugar')
    test = white_test ; train = white_train
  }
  
  if (full) {
    dat = train
  } else {
    dat = train[,which(names(train)%in% c(beta,'quality'))]  
  }
  
  fit = linear.mod(dat)
  betahat = fit$betahat
  
  ypred = round(as.matrix(cbind(1,test[,which(names(test)%in% rownames(betahat)[-1])]),nrow=nrow(test)) %*% betahat)
  result = data.frame(real = test$quality, pred = round(ypred))
  return(result)
}

# confusion matrix
linredresult = linrfac = lllin('red') ; linwhiteresult = linwfac = lllin('white') 
linrfac$real = factor(linredresult$real)
linrfac$pred = factor(linredresult$pred, levels=levels(linrfac$real))
linwfac$real = factor(linwhiteresult$real)
linwfac$pred = factor(linwhiteresult$pred, levels=levels(linwfac$real))

linredCM = confusionMatrix(data=linrfac$pred, reference=linrfac$real)
linwhiteCM = confusionMatrix(data=linwfac$pred, reference=linwfac$real)

# classification results (into quality score)
#cM.red$table
linrclass <- as.data.frame(linredCM$table)
linrclass$Freq[linrclass$Freq == 0] <-NA

#cM.white$table
linwclass <- as.data.frame(linwhiteCM$table)
linwclass$Freq[linwclass$Freq == 0] <-NA

# For TABLE
# prediction accuracy by category
linrpred <- round(diag(linredCM$table)/table(linrfac$real), 4)*100
linwpred <- round(diag(linwhiteCM$table)/table(linwfac$real), 4)*100

# prediction accuracy and kappa
linracc = round(linredCM$overall['Accuracy'], 4) * 100
linwacc = round(linwhiteCM$overall['Accuracy'], 4) * 100

# weighted kappas
wklin = wkappa(linwhiteCM$table)
rklin = wkappa(linredCM$table)

## FULL
linrfull = linrffac = lllin('red',full=FALSE) ; linwfull = linwffac = lllin('white',full=FALSE) 
linrffac$real = factor(linrfull$real)
linrffac$pred = factor(linrfull$pred, levels=levels(linrffac$real))
linwffac$real = factor(linwfull$real)
linwffac$pred = factor(linwfull$pred, levels=levels(linwffac$real))

linrfCM = confusionMatrix(data=linrffac$pred, reference=linrffac$real)
linwfCM = confusionMatrix(data=linwffac$pred, reference=linwffac$real)

# classification results (into quality score)
# For TABLE
# prediction accuracy by category
linrfpred <- round(diag(linrfCM$table)/table(linrffac$real), 4)*100
linwfpred <- round(diag(linwfCM$table)/table(linwffac$real), 4)*100

# prediction accuracy and kappa
linrfacc = round(linrfCM$overall['Accuracy'], 4) * 100
linwfacc = round(linwfCM$overall['Accuracy'], 4) * 100

# weighted kappas
wfklin = wkappa(linwfCM$table)
rfklin = wkappa(linrfCM$table)

lmresults <- setNames(data.frame(matrix(ncol = 10, nrow = 4)),
c("Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8","9"))
rownames(lmresults) <- c("Full (Red)","Reduced (Red)", "Full (White)", "Reduced (White)")
lmresults[2,] = c(linrfacc, round(rfklin$kappa,4), round(rfklin$weighted.kappa,4),linrfpred,NA)
lmresults[1,] = c(linracc, round(rklin$kappa,4), round(rklin$weighted.kappa,4),linrpred,NA)
lmresults[4,] = c(linwfacc, round(wfklin$kappa,4), round(wfklin$weighted.kappa,4),linwfpred)
lmresults[3,] = c(linwacc, round(wklin$kappa,4), round(wklin$weighted.kappa,4),linwpred)

kable(lmresults, booktabs=T,digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))

```

Below is a visualization of the confusion matrix for red wine. We can see that the linear model tends to predict most of the quality towards the mean value since the model captures the population mean and also the data is highly unbalanced with most of the data concentrated around the 5 and 6 quality ratings. 
```{r, echo=F, fig.align='center'}
# classification results as a plot
cmPlot(linrclass, "red", pred_first = TRUE,
       "Linear Model Classification of Quality for Red Wines", axis.title.size = 20, axis.text.size = 25)
```

This concentration towards the mean trend is more distinct with the white wine, where most of the prediction is 6.
```{r whitelinplot, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
cmPlot(linwclass, "white", pred_first = TRUE,
      "Linear Model Classification of Quality for White Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Partial Proportional Odds Model (White Wine)

  For models fit on the white wine training data, only proportional odds models converged to parameter estimates that produced non-negative fitted probabilities for each subject and quality rating. The code for fitting these models is given below (note that the models objects are saved to shorten the compile time for this report):

```{r, echo=T, warning=F, message = F, results = 'hide', eval=F, fig.align='center'}

#get starting values for model beta's
beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + residual.sugar,
                       data = white_train))

#proportional odds, reduced model 
white.prop.odds <- partial.prop.odds.mod(y ="quality", in.data = white_train,
                              prop.odds.formula = ~ alcohol+ pH + volatile.acidity + residual.sugar,
                              beta.prop.odds.start = beta.starts[c(2:5)],
                              method = "BFGS")
saveRDS(white.prop.odds, "proportional_odds_models/white.prop.odds.reduced.rds")


#proportional odds, full model 
beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + residual.sugar + fixed.acidity + citric.acid +
                         chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + sulphates,
                       data = white_train))

white.prop.odds <- partial.prop.odds.mod(y ="quality", in.data = white_train,
                              prop.odds.formula = ~ alcohol+ pH + volatile.acidity + residual.sugar +
                                fixed.acidity + citric.acid + chlorides + free.sulfur.dioxide + 
                                total.sulfur.dioxide + density + sulphates,
                              beta.prop.odds.start = beta.starts[c(2:12)],
                              method = "BFGS")

saveRDS(white.prop.odds, "proportional_odds_models/white.prop.odds.rds")

```

Then using the predict method implemented for this class, we predicted the ratings in the testing data as follows:

```{r, echo=T, warning=F, message = F}
#load saved models 
white.prop.odds.reduced <- readRDS("proportional_odds_models/white.prop.odds.reduced.rds")
white.prop.odds <- readRDS("proportional_odds_models/white.prop.odds.rds")

#predict values 
white.preds <- predict(white.prop.odds, white_test)$most.likely
white.preds.reduced <- predict(white.prop.odds.reduced, white_test)$most.likely

```

The performance for the full and reduced models are given below. Notably, the full model performed better than the reduced, but only very slightly. 

```{r, echo=F, warning=F, message = F}
#compare to actual values  
white.pred.table <- data.frame(table(factor(white.preds, levels = sort(unique(white.obs))), white.obs))
white.level.accuracy <- sapply(sort(unique(white.obs)), function(x){
  
  target <- white.pred.table[ white.pred.table$white.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$white.obs])
  100*correct/total
  
})
white.pred.table$Freq[white.pred.table$Freq == 0] <- NA
white.pred.accuracy <- 100*sum(white.preds == white.obs)/length(white.obs)
white.pred.kappa <- cohen.kappa(cbind(white.preds, white.obs))$kappa
white.pred.wkappa <- cohen.kappa(cbind(white.preds, white.obs))$weighted.kappa
white.prop.odds.metrics <- c(white.pred.accuracy, white.pred.kappa, white.pred.wkappa,
                              white.level.accuracy)

#also for reduced model 
white.pred.table.reduced <- data.frame(table(factor(white.preds.reduced, levels = sort(unique(white.obs))), white.obs))
white.level.accuracy.reduced <- sapply(sort(unique(white.obs)), function(x){
  
  target <- white.pred.table.reduced[ white.pred.table.reduced$white.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$white.obs])
  100*correct/total
  
})
white.pred.table.reduced$Freq[white.pred.table.reduced$Freq == 0] <- NA
white.pred.accuracy.reduced <- 100*sum(white.preds.reduced == white.obs)/length(white.obs)
white.pred.kappa.reduced <- cohen.kappa(cbind(white.preds.reduced, white.obs))$kappa
white.pred.wkappa.reduced <- cohen.kappa(cbind(white.preds.reduced, white.obs))$weighted.kappa
white.prop.odds.metrics.reduced <- c(white.pred.accuracy.reduced , white.pred.kappa.reduced,
                                     white.pred.wkappa.reduced, white.level.accuracy.reduced)

#model comp
prop.odds.white.comp <- setNames(data.frame(matrix(ncol = 10, nrow = 2)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8", "9"))
rownames(prop.odds.white.comp) <- c("Proportional Odds (F)", "Proportional Odds (R)")

prop.odds.white.comp[1,] <- white.prop.odds.metrics
prop.odds.white.comp[2, ] <- white.prop.odds.metrics.reduced

kable(prop.odds.white.comp, booktabs=T,digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))

```

Likewise, the confusion matrix for the full model, using the plotting functionality implemented in our package, is given below:

```{r, echo= T, warning=F, message=F, fig.align='center'}
#plotting confusion matrix 
cmPlot(white.pred.table, "white", pred_first = T, title = "Proportional Odds Model Predicted vs Observed", axis.title.size = 20, axis.text.size = 25)

```

## Results: Partial Proportional Odds Model (Red Wine)
 
 For models fit to the red training data, both proportional and partial proportional models converged. The results presented for the partial proportional model allow the coefficient for total sulfur dioxide to vary with the level of wine quality. The code for producing these models is given below:

```{r,echo=T, warning=F, message = F, results='hide', eval=F, fig.align='center'}

#reduced red wine models 
beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + sulphates + total.sulfur.dioxide,
                       data = red_train))

red.partial.prop.reduced <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol + pH+ volatile.acidity + sulphates,
                              beta.prop.odds.start = beta.starts[2:5],
                              non.prop.odds.formula = ~total.sulfur.dioxide,
                              beta.non.prop.odds.start = matrix(rep(beta.starts[6], 5), nrow = 1),
                              method = "BFGS")

red.prop.odds.reduced <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol + pH + volatile.acidity + sulphates
                              + total.sulfur.dioxide,
                              beta.prop.odds.start = beta.starts[2:6],
                              method = "BFGS")
saveRDS(red.partial.prop.reduced, "proportional_odds_models/red.partial.prop.reduced.rds")
saveRDS(red.prop.odds.reduced, "proportional_odds_models/red.prop.odds.reduced.rds")


#full models 
beta.starts <- coef(lm(quality ~ alcohol+ pH + volatile.acidity + residual.sugar + fixed.acidity + citric.acid +
                         chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + sulphates,
                       data = red_train))

#partial proportional odds model
red.partial.prop <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol+ pH + volatile.acidity + residual.sugar +
                                fixed.acidity + citric.acid + chlorides + free.sulfur.dioxide +
                                density + sulphates,
                              beta.prop.odds.start = beta.starts[c(2:9, 11:12)],
                              non.prop.odds.formula = ~total.sulfur.dioxide,
                              beta.non.prop.odds.start = matrix(rep(beta.starts[10], 5), nrow = 1),
                              method = "BFGS")

#proportional odds model
red.prop.odds <- partial.prop.odds.mod(y ="quality", in.data = red_train,
                              prop.odds.formula = ~ alcohol+ pH + volatile.acidity + residual.sugar +
                                fixed.acidity + citric.acid + chlorides + free.sulfur.dioxide +
                                total.sulfur.dioxide + density + sulphates,
                              beta.prop.odds.start = beta.starts[2:12],
                              method = "BFGS")

saveRDS(red.partial.prop, "proportional_odds_models/red.partial.prop.rds")
saveRDS(red.prop.odds, "proportional_odds_models/red.prop.odds.rds")

```

Predictions on the testing data were again made using our S3 predict method (code as shown for white wines). The performance of the trained models on the testing data are given below. Briefly, the proportional and partial proportional odds models performed similarly, and the full models performed very slightly better than the reduced models. 

```{r,echo=F, warning=F, message = F, results='hide', fig.align='center'}

#load saved models 
red.partial.prop <- readRDS("proportional_odds_models/red.partial.prop.rds")
red.prop.odds <- readRDS("proportional_odds_models/red.prop.odds.rds")
red.partial.prop.reduced <- readRDS("proportional_odds_models/red.partial.prop.reduced.rds")
red.prop.odds.reduced <- readRDS("proportional_odds_models/red.prop.odds.reduced.rds")

#### Full models 

#partial proportional odds model predictions/metrics
red.partial.prop.preds <- predict(red.partial.prop, red_test)$most.likely
red.partial.prop.preds.table <- data.frame(table(factor(red.partial.prop.preds, levels = sort(unique(red.obs))), red.obs))
red.partial.prop.level.accuracy <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.partial.prop.preds.table[ red.partial.prop.preds.table$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.partial.prop.preds.table$Freq[red.partial.prop.preds.table$Freq == 0] <- NA
red.partial.prop.accuracy <- 100*sum(red.partial.prop.preds == red.obs)/length(red.obs)
red.partial.prop.kappa <- cohen.kappa(cbind(red.partial.prop.preds, red.obs))$kappa
red.partial.prop.wkappa <- cohen.kappa(cbind(red.partial.prop.preds, red.obs))$weighted.kappa
red.partial.prop.metrics <- c(red.partial.prop.accuracy, red.partial.prop.kappa, red.partial.prop.wkappa,
                              red.partial.prop.level.accuracy)

#proportional odds model predictions/metrics
red.prop.odds.preds <- predict(red.prop.odds, red_test)$most.likely

red.prop.odds.table <- data.frame(table(factor(red.prop.odds.preds, levels = sort(unique(red.obs))), red.obs))
red.prop.odds.level.accuracy <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.prop.odds.table[ red.prop.odds.table$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.prop.odds.table$Freq[red.prop.odds.table$Freq == 0] <- NA
red.prop.odds.accuracy <- 100*sum(red.prop.odds.preds == red.obs)/length(red.obs)
red.prop.odds.kappa <- cohen.kappa(cbind(red.prop.odds.preds, red.obs))$kappa
red.prop.odds.wkappa <- cohen.kappa(cbind(red.prop.odds.preds, red.obs))$weighted.kappa
red.prop.odds.metrics <- c(red.prop.odds.accuracy, red.prop.odds.kappa, red.prop.odds.wkappa, red.prop.odds.level.accuracy)

#### Reduced models 

#partial proportional odds 
red.partial.prop.preds.reduced <- predict(red.partial.prop.reduced, red_test)$most.likely
red.partial.prop.preds.table.reduced <- data.frame(table(factor(red.partial.prop.preds.reduced, levels = sort(unique(red.obs))), red.obs))
red.partial.prop.level.accuracy.reduced <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.partial.prop.preds.table.reduced[ red.partial.prop.preds.table.reduced$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.partial.prop.preds.table.reduced$Freq[red.partial.prop.preds.table.reduced$Freq == 0] <- NA
red.partial.prop.accuracy.reduced <- 100*sum(red.partial.prop.preds.reduced == red.obs)/length(red.obs)
red.partial.prop.kappa.reduced <- cohen.kappa(cbind(red.partial.prop.preds.reduced, red.obs))$kappa
red.partial.prop.wkappa.reduced <- cohen.kappa(cbind(red.partial.prop.preds.reduced, red.obs))$weighted.kappa
red.partial.prop.metrics.reduced <- c(red.partial.prop.accuracy.reduced, red.partial.prop.kappa.reduced,
                                      red.partial.prop.wkappa.reduced,red.partial.prop.level.accuracy.reduced)


## proportional odds 
red.prop.odds.preds.reduced <- predict(red.prop.odds.reduced, red_test)$most.likely

red.prop.odds.table.reduced <- data.frame(table(factor(red.prop.odds.preds.reduced, levels = sort(unique(red.obs))), red.obs))
red.prop.odds.level.accuracy.reduced <- sapply(sort(unique(red.obs)), function(x){
  
  target <- red.prop.odds.table.reduced[ red.prop.odds.table.reduced$red.obs == x, ]
  total <- sum(target$Freq) 
  correct <- sum(target$Freq[target$Var1 == target$red.obs])
  100*correct/total
  
})
red.prop.odds.table.reduced$Freq[red.prop.odds.table.reduced$Freq == 0] <- NA
red.prop.odds.accuracy.reduced <- 100*sum(red.prop.odds.preds.reduced == red.obs)/length(red.obs)
red.prop.odds.kappa.reduced <- cohen.kappa(cbind(red.prop.odds.preds.reduced, red.obs))$kappa
red.prop.odds.wkappa.reduced <- cohen.kappa(cbind(red.prop.odds.preds.reduced, red.obs))$weighted.kappa
red.prop.odds.metrics.reduced <- c(red.prop.odds.accuracy.reduced, red.prop.odds.kappa.reduced,
                                   red.prop.odds.wkappa.reduced, red.prop.odds.level.accuracy.reduced)

```

```{r, echo=F}

prop.odds.red.comp <- setNames(data.frame(matrix(ncol = 9, nrow = 4)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8"))
rownames(prop.odds.red.comp) <- c("Proportional Odds (F)", "Proportional Odds (R)", "Partial Proportional Odds (F)", "Partial Proportional Odds (R)")

prop.odds.red.comp[1,] <- red.prop.odds.metrics
prop.odds.red.comp[2, ] <- red.prop.odds.metrics.reduced
prop.odds.red.comp[3, ] <- red.partial.prop.metrics
prop.odds.red.comp[4, ] <- red.partial.prop.metrics.reduced


kable(prop.odds.red.comp, booktabs=T,digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 6)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))

```

  As with the white wines, the confusion matrices for the full proportional and partial proportional red models are shown below:

```{r, echo=F, warning=F, message = F, fig.align='center'}
#plotting confusion matrix 
cmPlot(red.prop.odds.table, "red", pred_first = T, title = "Proportional Odds Predicted vs Observed (Full Model)", axis.title.size = 20, axis.text.size = 25)
```

```{r, echo=F, warning=F, message = F, fig.align='center'}
cmPlot(red.partial.prop.preds.table, "red", pred_first = T, title = "Partial Proportional Odds Predicted vs Observed (Full)", axis.title.size = 20, axis.text.size = 25)
```

## Results: Multinomial Regression (Red Wine Quality Classification)

Here we fit the multinomial regression models using the `fit_multinomial_regression` function from the `sommelieR` package. This chunk isn't evaluated for ease of compiling the final report - but obviously would be in a paper that was more concerned with reproducibility. 
```{r echo=TRUE, eval = F, message=F, results='hide'}
#Begin multinomiial Results Section

set.seed(13)

#Fit full model for red wine
red_train_full_fit <- fit_multinomial_regression(red_train, quality ~ 1 + ., ref_level = "8", trace = 1, itters = 300)

#Fit reduced linear for red wine
red_train_reduced_linear_fit <- fit_multinomial_regression(red_train, quality ~ 1 + alcohol + volatile.acidity + total.sulfur.dioxide + pH + sulphates, ref_level = "8", trace = 1, itters = 300)

#Fit reduced quadratic for red wine
red_train_reduced_all_quad_fit <- fit_multinomial_regression(red_train, quality ~ 1 + alcohol + volatile.acidity + total.sulfur.dioxide + pH+ I(alcohol*alcohol) + I(volatile.acidity*volatile.acidity) + I(pH*pH) + I(total.sulfur.dioxide*total.sulfur.dioxide) + I(sulphates*sulphates) + (alcohol + volatile.acidity + total.sulfur.dioxide + pH + sulphates)^2, ref_level = "8", trace = 1, itters = 300)

#Fit full linear for white wine
white_train_full_fit <- fit_multinomial_regression(white_train, quality ~ 1 + ., ref_level = "8", trace = 1, itters = 300)

#Fit reduced lienar for white wine
white_train_linear_fit <- fit_multinomial_regression(white_train, quality ~ 1 + pH + volatile.acidity + residual.sugar + alcohol, ref_level = "8", trace = 1, itters = 300)

#Fit reduced with quadratic terms for white wine
#Takes a long time to run
white_train_quad_fit <- fit_multinomial_regression(white_train, quality ~ 1 + pH + volatile.acidity + residual.sugar + alcohol + (pH + volatile.acidity + residual.sugar + alcohol)^2 + I(pH*pH) + I(volatile.acidity*volatile.acidity) + I(residual.sugar*residual.sugar) + I(alcohol*alcohol), ref_level = "8", trace = 1, itters = 300)

#Save models because they take a while to run
saveRDS(red_train_full_fit, "multinomial_models/red_full_fit.rds")
saveRDS(red_train_reduced_linear_fit, "multinomial_models/red_reduced_linear_fit.rds")
saveRDS(red_train_reduced_all_quad_fit, "multinomial_models/red_reduced_all_quad_fit.rds")
saveRDS(white_train_full_fit, "multinomial_models/white_full_fit.rds")
saveRDS(white_train_linear_fit, "multinomial_models/white_reduced_linear_fit.rds")
saveRDS(white_train_quad_fit, "multinomial_models/white_reduced_quad_fit.rds")
```


```{r message=F, include=FALSE, results='hide'}
red_train_full_fit <- readRDS("multinomial_models/red_full_fit.rds")
red_train_reduced_linear_fit <- readRDS("multinomial_models/red_reduced_linear_fit.rds")
red_train_reduced_all_quad_fit <- readRDS("multinomial_models/red_reduced_all_quad_fit.rds")
white_train_full_fit <- readRDS("multinomial_models/white_full_fit.rds")
white_train_linear_fit <- readRDS("multinomial_models/white_reduced_linear_fit.rds")
white_train_quad_fit <- readRDS("multinomial_models/white_reduced_quad_fit.rds")
```

Here we fit predictions on the test set using the `predict_multinomial` function from the `sommelieR` package. 
```{r echo=TRUE, message=F, results='hide'}
#Fit predictions
red_test_full_preds <- predict_multinomial(red_train_full_fit, red_test)
red_test_reduced_linear_preds <- predict_multinomial(red_train_reduced_linear_fit, red_test)
red_test_reduced_quad_preds <- predict_multinomial(red_train_reduced_all_quad_fit, red_test)
white_test_full_preds <- predict_multinomial(white_train_full_fit, white_test)
white_test_linear_preds <- predict_multinomial(white_train_linear_fit,
                                              white_test)
white_test_quad_preds <- predict_multinomial(white_train_quad_fit, white_test)
```

And here we use a convenient helper function `pred_summary_stats` to assemble our final table. It's from the `sommelieR` package - soon to be on CRAN.
```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
multinomial_red_results <- bind_rows(
  pred_summary_stats(red_test_full_preds, red_test$quality, "Full Model (Linear Terms)"),
  pred_summary_stats(red_test_reduced_linear_preds, red_test$quality, "Reduced Model (Linear Terms)"),
   pred_summary_stats(red_test_reduced_quad_preds, red_test$quality, "Reduced Model (Second Order Terms)")
) %>% dplyr::arrange(desc(`Weighted Kappa`)) 
colnames(multinomial_red_results)[2] <- "Accuracy"

multinomial_red_results %>% 
  kable(caption = "Comparison of Multinomial Regression Models for Red Wine Quality", 
        booktabs = T, digits = 4) %>%
        add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 6)) %>%
        kable_styling(latex_options = c("repeat_header", "scale_down"))

#Remove "model" because it's not in the big table. 
multinomial_best_red_model_results <- multinomial_red_results %>% slice(1) %>% 
  select(-Model)
```

We see from the above table that the full model with linear predictors has the best weighted kappa of all considered multinomial regression models for red wine. It is our best model and will represent multinomial regression in the final comparison. Its confusion matrix is visualized below:

```{r, echo=F, warning=F, message = F, echo=F, fig.align='center'}
multinom_red_best_cm <- confusionMatrix(as.factor(red_test_full_preds), as.factor(red_test$quality))$table %>% 
  as.data.frame()

multinom_red_best_cm$Freq[multinom_red_best_cm$Freq == 0] <-NA
cmPlot(multinom_red_best_cm, "red", pred_first = T, title = "Multinomial Regression Confused\n on Non-Average Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Multinomial Regression (White Wine Quality Classification)
The same functions were again used to construct the table:
```{r echo=TRUE, message=FALSE, warning=FALSE}
multinomial_white_results <- bind_rows(
  pred_summary_stats(white_test_full_preds, white_test$quality, "Full Model (Linear Terms)"),
  pred_summary_stats(white_test_linear_preds, white_test$quality, "Reduced Model (Linear Terms)"),
   pred_summary_stats(white_test_quad_preds, white_test$quality, "Reduced Model (Second Order Terms)"))%>% 
  dplyr::arrange(desc(`Weighted Kappa`))
colnames(multinomial_white_results)[2] <- "Accuracy"

multinomial_white_results %>% 
  kable(caption = "Comparison of Multinomial Regression Models for White Wine Quality", 
        booktabs = T, digits = 4) %>%
        add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
        kable_styling(latex_options = c("repeat_header", "scale_down"))

multinomial_white_best_model_results <- multinomial_white_results %>%
  slice(1) %>% 
  select(-Model)
```

We see from the above table that the full model with linear predictors has the best weighted kappa of all considered multinomial regression models for white wine. It is our best model and will represent multinomial regression in the final comparison for white wines. Its confusion matrix is visualized below:

## Results: Multinomial Regression - Full Model Confusion Matrix (White Wine)
```{r, echo=F, warning=F, message = F, fig.align='center'}
multinom_white_best_cm <- confusionMatrix(as.factor(white_test_full_preds), as.factor(white_test$quality))$table %>% 
  as.data.frame()

multinom_white_best_cm$Freq[multinom_white_best_cm$Freq == 0] <-NA
cmPlot(multinom_white_best_cm, "white", pred_first = T, title = "Multinomial Regression Confused\n on Non-Average Wines", axis.title.size = 20, axis.text.size = 25)
```

## Results: Random Forest
The results of the random forest model for red wine and white wine are displayed in the table below. We find that the random forest has decent prediction accuracy, but in each case fails to classify any wines into the lowest or highest quality cateogry.
```{r red.rf, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
#source("./R/cmPlot_function.R") # required to plot CMs
# train random forest
trCtl <- trainControl(savePredictions=TRUE)
#fit.red <- train(x.train.red, y.train.red, method="rf", trControl=trCtl)
#fit.white <- train(x.train.white, y.train.white, method="rf", trControl=trCtl)
# saveRDS(fit.red, "rf_models/rf_red_fit.rds")
# saveRDS(fit.white, "rf_models/rf_white_fit.rds")
fit.red <- readRDS("rf_models/rf_red_fit.rds")
fit.white <- readRDS("rf_models/rf_white_fit.rds")

# random forest training results
#fit.red$results
#fit.white$results

# use training model to predict y
y.pred.red <- predict(fit.red, x.test.red)
y.pred.white <- predict(fit.white, x.test.white)

# results of rf on test data
cM.red = confusionMatrix(data=y.pred.red, reference=y.test.red)
cM.white = confusionMatrix(data=y.pred.white, reference=y.test.white)

# classification results (into quality score)
#cM.red$table
class.red <- as.data.frame(cM.red$table)
class.red$Freq[class.red$Freq == 0] <-NA

#cM.white$table
class.white <- as.data.frame(cM.white$table)
class.white$Freq[class.white$Freq == 0] <-NA

# prediction accuracy by category
pred.red <- round(diag(cM.red$table)/table(y.test.red), 4)*100
pred.white <- round(diag(cM.white$table)/table(y.test.white), 4)*100

# prediction accuracy and kappa
res.red = round(cM.red$overall[1:2], 4)
res.red[1] = res.red[1]*100
res.white = round(cM.white$overall[1:2], 4) 
res.white[1] = res.white[1]*100

# weighted kappas
wk.red <- round(cohen.kappa(cM.red$table)$weighted.kappa, 4)
wk.white <- round(cohen.kappa(cM.white$table)$weighted.kappa, 4)

# print results
rf.results <- setNames(data.frame(matrix(ncol = 10, nrow = 2)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8", "9"))
rownames(rf.results) <- c("Red Wine", "White Wine")

pred.red2 <- c(pred.red, NA)
rf.results[1,] <- c(res.red, wk.red, pred.red2)
rf.results[2,] <- c(res.white, wk.white, pred.white)

kable(rf.results, caption = "Random Forest Results for Red and White Wine", booktabs=T, digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

Below are plots of the mean decrease gini values demonstrating varible importance in each of the red and white wine random forest models. In both cases, the alcohol content's mean decrease gini value is indicative of high variable importance.

```{r varimp_rf, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.width='50%'}
red.imp <- fit.red$finalModel$importance
white.imp <- fit.white$finalModel$importance
par(mfrow=c(1,2))
dotplot(sort(red.imp[,1]), xlab="Mean Decrease Gini", main="Variable Importance - Red Wine")
dotplot(sort(white.imp[,1]), xlab="Mean Decrease Gini", main="Variable Importance - White Wine")
```

The confusion matrices are visualized below:

```{r, echo=F, fig.align='center'}
# classification results as a plot
cmPlot(class.red, "red", pred_first = TRUE,
"RF Classification of Quality for Red Wines", axis.title.size = 20, axis.text.size = 25)
```


```{r white.rf, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
cmPlot(class.white, "white", pred_first = TRUE,
"RF Classification of Quality for White Wines", axis.title.size = 20, axis.text.size = 25)
```

## Discussion    

Comparisons of the performance of the different modeling approaches are given below:

#### Comparison of Results for Red Wine
```{r echo=FALSE, message=FALSE, warning=FALSE}
#-----------------------------------------------------------------------------#
#-----------------------------------------------------------------------------#
# Create Results Data Tables
# round results to 4 digits
# accuracy into categories as percent
# rename row and fill in data accordingly (add more rows if needed)

#-----------------------------------------------------------------------------#
# RED WINE NO GROUPING
results.red <- setNames(data.frame(matrix(ncol = 9, nrow = 5)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8"))
rownames(results.red) <- c("Random Forest", "Proportional Odds", "Partial Proportional Odds","Multinomial", "Linear Regression")

results.red[1,] <- c(res.red, wk.red, pred.red)
results.red[2, ] <- red.prop.odds.metrics
results.red[3, ] <- red.partial.prop.metrics
results.red["Multinomial", ] <- multinomial_best_red_model_results
results.red['Linear Regression', ] <- c(linracc, round(rklin$kappa,4), round(rklin$weighted.kappa,4),linrpred)


#-----------------------------------------------------------------------------#
# WHITE WINE NO GROUPING
results.white <- setNames(data.frame(matrix(ncol = 10, nrow = 4)),
c("Prediction Accuracy", "Kappa", "Weighted Kappa", "3", "4", "5", "6", "7", "8", "9"))
rownames(results.white) <- c("Random Forest", "Proportional Odds", "Multinomial", "Linear Regression")

results.white[1,] <- c(res.white, wk.white, pred.white)
results.white[2, ] <- white.prop.odds.metrics 
results.white["Multinomial", ] <- multinomial_white_best_model_results
results.white['Linear Regression', ] <- c(linwacc, round(wklin$kappa,4), round(wklin$weighted.kappa,4),linwpred)

#-----------------------------------------------------------------------------#
# Print out comparison tables created above
# Red wine no groups

results.red %>% rownames_to_column("Model") %>%
  arrange(desc(`Weighted Kappa`)) %>% 
  kable(booktabs=T,digits = 4) %>%
  add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 6)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))

#-----------------------------------------------------------------------------#
```

#### Comparison of Results: White Wine
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Print out comparison tables created above

# White wine no groups
results.white %>% rownames_to_column("Model")%>%
  arrange(desc(`Weighted Kappa`)) %>% 
kable(booktabs=T, digits = 4) %>%
add_header_above(c(" ", "Overall Results" = 3, "Percent Correct by Category" = 7)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

  Using random forests, we predicted expert wine quality ratings fairly accurately. For white wines, the overall accuracy was `r res.white[1]`%, with a weighted kappa of `r wk.white`. For red wines, the accuracy was `r res.red[1]`% with weighted kappa `r wk.red`. Using the proposed cutoffs from Landis and Koch, these weighted kappa values suggest moderate to substantial agreement with the expert ratings. 
  
  The likelihood approaches performed more poorly. Accuracies were 10-15% lower than random forest and weighted kappas were 0.1 to 0.2 lower. Moreover, among the likelihood based models, predictive performance was similar and no individual approach stood out as superior. Generally, the models showed a tendency to predict ratings towards the middle of the distribution, and, interestingly, accounting for the ordered nature of the ratings did not make a substantial difference in performance. Indeed, the multinomial was more accurate than the proportional odds and linear regression models for very low and high quality wines. Overall, the weighted kappas for the likelihood based models suggested moderate agreement with expert ratings.  
  
  Our project has limitations. We only had data on a small number of chemical components, so we speculate some misclassification was due to failure to include other important chemicals in our models. Likewise, our datasets included only small numbers of extremely poor and very excellent wines making these difficult to predict. In the future, it would be worthwhile to examine whether various outlier detection algorithms are more suited to characterizing these wines than the approaches we took. 
  
  In terms of variable selection for the likelihood based models, we saw only a minor improvement in predictive performance for the full compared to reduced models suggesting we may have overfit the data. A regularization method like LASSO or elastic net with cross-validation may have been useful. However, it is somewhat unclear how best to implement a penalization method for the partial proportional odds models and the multinomial regression models - since each coefficient for a given class and variable is related to the other coefficients for a class. Therefore, some sort of grouped variable selection method would need to be implemented for these models. Additionally, our modeling focus was purely on prediction, not on characterizing how individual chemicals relate to wine quality. An interesting extension of this project would more thoroughly investigate those relationships. 
  
  In summary, we found wine quality ratings can be predicted reasonably well based on their chemical components. Random forests performed better than likelihood based models, but all methods demonstrated at least moderate agreement with expert ratings. Nevertheless, the predictions were far from perfect, so true wine connoisseurs are still better off consulting a sommelier. 

## References

Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. 1977 Mar;33(1):159-74.

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.



